# ImplementaciÃ³n de mÃºltiples modelos para la detecciÃ³n del cÃ¡ncer de mama  
### Un enfoque prÃ¡ctico mediante Python y tÃ©cnicas de aprendizaje supervisado  
**Autores:** Maximiliano Angelini â€“ AndrÃ©s Miranda  
**Departamento de MatemÃ¡ticas, UTFSM**

---

## ğŸ“„ Poster del Proyecto

<p align="center">
  <a href="poster/Poster.pdf">
    <img src="poster/Poster_page-0001.jpg" width="100%" alt="Poster del proyecto">
  </a>
</p>

ğŸ‘‰ *Haz clic sobre la imagen para ver el poster completo en PDF.*

---

## ğŸ“Œ DescripciÃ³n del proyecto

Este repositorio contiene el cÃ³digo, figuras y resultados del estudio aplicado al  
**Breast Cancer Wisconsin Diagnostic Dataset**, un conjunto de datos clÃ¡sico para la clasificaciÃ³n de tumores *benignos* y *malignos*.

El objetivo del proyecto es:

- Analizar la estructura del dataset mediante tÃ©cnicas exploratorias  
- Reducir dimensionalidad seleccionando covariables clave  
- Entrenar mÃºltiples modelos de clasificaciÃ³n  
- Comparar su rendimiento usando validaciÃ³n cruzada y mÃ©tricas ROC  
- Elegir un modelo final con alta precisiÃ³n e interpretabilidad clÃ­nica

El trabajo completo estÃ¡ resumido en el poster del curso MAT281 â€“ Aplicaciones de la MatemÃ¡ticas en IngenierÃ­a.

---

## ğŸ§¬ Dataset

Se utiliza el **Breast Cancer Wisconsin Diagnostic Dataset**, el cual contiene:

- **569 observaciones**
- **30 atributos cuantitativos** derivados de imÃ¡genes digitales de nÃºcleos celulares

Las covariables describen forma, textura e irregularidad celular, permitiendo diferenciar tumores:

- **Benignos:** mÃ¡s regulares, variabilidad baja  
- **Malignos:** estructuras irregulares, mayor dispersiÃ³n  

---

## ğŸ” AnÃ¡lisis Exploratorio

Incluye:

- Matriz de correlaciÃ³n  
- Histogramas por covariable  
- AnÃ¡lisis de Componentes Principales (PCA)

Principales hallazgos:

- Alta colinealidad entre *radius*, *area*, *perimeter*, *concave points*, *compactness* y *concavity*  
- La **primera componente principal** explica â‰ˆ **44 %** de la variabilidad  
- Las **primeras 10 componentes** explican â‰ˆ **95 %**

Esto motiva la **reducciÃ³n de dimensionalidad**, manteniendo variables esenciales como *radius* y *concave points*.

---

## ğŸ§  Modelos Implementados

Se evaluaron los siguientes modelos clÃ¡sicos de clasificaciÃ³n:

- Logistic Regression  
- Decision Tree  
- Random Forest (200 trees)  
- Quadratic Discriminant Analysis  
- Linear Discriminant Analysis  
- SVC (linear, sigmoid, rbf)  
- GradientBoostingClassifier (100 estimadores, depth=3)

La evaluaciÃ³n se realizÃ³ mediante **ValidaciÃ³n Cruzada**.

---

## ğŸ“Š Resultados Principales

- Todos los modelos obtienen *alta precisiÃ³n*, donde el mayor AUC â‰ˆ **0.99**  
- La fuerte separabilidad entre clases permite simplificar el modelo sin perder rendimiento  
- Modelos lineales â€”especialmente **RegresiÃ³n LogÃ­stica** y **SVM lineal**â€” destacan por su estabilidad y claridad interpretativa

Para el conjunto de prueba (80/20), la **RegresiÃ³n LogÃ­stica (Data 1)** obtiene:

- **Sensibilidad:** 92.86 %  
- **Especificidad:** 100 %  

---

## ğŸ–¼ï¸ Resultados Visuales

A continuaciÃ³n se presentan las figuras mÃ¡s relevantes generadas durante el anÃ¡lisis.

---

### ğŸ”¹ Curvas ROC por modelo

<p align="center">
  <img src="figures/RocCurves.png" width="95%">
</p>

---

### ğŸ”¹ Matrices de ConfusiÃ³n

<p align="center">
  <img src="figures/ConfusionMatrix.png" width="95%">
</p>

---

### ğŸ”¹ Matriz de CorrelaciÃ³n

<p align="center">
  <img src="figures/Correlation_Matrix.png" width="95%">
</p>

---

### ğŸ”¹ Histogramas por Covariable

<p align="center">
  <img src="figures/Histogramas.png" width="95%">
</p>

---

### ğŸ”¹ Componentes Principales (PCA)

<p align="center">
  <img src="figures/PCA.png" width="95%">
</p>

---
