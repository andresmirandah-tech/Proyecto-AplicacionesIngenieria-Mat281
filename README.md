# Implementaci√≥n de m√∫ltiples modelos para la detecci√≥n del c√°ncer de mama  
### Un enfoque pr√°ctico mediante Python y t√©cnicas de aprendizaje supervisado  
**Autores:** Maximiliano Angelini ‚Äì Andr√©s Miranda  
**Departamento de Matem√°ticas, UTFSM**

---

## üìå Descripci√≥n del proyecto

Este repositorio contiene el c√≥digo, figuras y resultados del estudio aplicado al  
**Breast Cancer Wisconsin Diagnostic Dataset**, un conjunto de datos cl√°sico para la clasificaci√≥n de tumores *benignos* y *malignos*.

El objetivo del proyecto es:

- Analizar la estructura del dataset mediante t√©cnicas exploratorias  
- Reducir dimensionalidad seleccionando covariables clave  
- Entrenar m√∫ltiples modelos de clasificaci√≥n  
- Comparar su rendimiento usando validaci√≥n cruzada y m√©tricas ROC  
- Elegir un modelo final con alta precisi√≥n e interpretabilidad cl√≠nica

El trabajo completo est√° resumido en el poster del curso MAT281 ‚Äì Aplicaciones de la Matem√°ticas en Ingenier√≠a.

---

## üß¨ Dataset

Se utiliza el **Breast Cancer Wisconsin Diagnostic Dataset**, el cual contiene:

- **569 observaciones**
- **30 atributos cuantitativos** derivados de im√°genes digitales de n√∫cleos celulares
- 
Las covariables describen forma, textura e irregularidad celular, permitiendo diferenciar tumores:

- **Benignos:** m√°s regulares, variabilidad baja  
- **Malignos:** estructuras irregulares, mayor dispersi√≥n  

---

## üîç An√°lisis Exploratorio

Incluye:

- Matriz de correlaci√≥n  
- Histogramas por covariable  
- An√°lisis de Componentes Principales (PCA)

Principales hallazgos:

- Alta colinealidad entre radius, area, perimeter, concave points, compactness y concavity  
- La **primera componente principal** explica ‚âà **44 %** de la variabilidad  
- Las **primeras 10 componentes** explican ‚âà **95 %**

Esto motiva la **reducci√≥n de dimensionalidad** manteniendo variables esenciales como *radius* y *concave points*.

---

## üß† Modelos Implementados

Se evaluaron los siguientes modelos cl√°sicos de clasificaci√≥n:

- Logistic Regression  
- Decision Tree  
- Random Forest (200 trees)  
- Quadratic Discriminant Analysis  
- Linear Discriminant Analysis  
- SVC (linear, sigmoid, rbf)  
- GradientBoostingClassifier (100 estimadores, depth=3)

La evaluaci√≥n se realiz√≥ mediante **Validaci√≥n Cruzada**.

---

## üìä Resultados Principales

- Todos los modelos obtienen *alta precisi√≥n*, donde el mayor AUC ‚âà **0.99**  
- La fuerte separabilidad entre clases permite simplificar el modelo sin perder rendimiento  
- Modelos lineales ‚Äîespecialmente **Regresi√≥n Log√≠stica** y **SVM lineal**‚Äî destacan por su estabilidad y claridad interpretativa

Para el conjunto de prueba (80/20), la **Regresi√≥n Log√≠stica (Data 1)** obtiene:

- **Sensibilidad:** 92.86 %  
- **Especificidad:** 100 %  

---


